{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b5d8c50",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c4b56d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_START_FROM_SCRATCH = True\n",
    "DOCKER_INTERNAL_HOST = \"host.docker.internal\"\n",
    "DOCKER_DNS = [\"10.15.20.1\"]\n",
    "\n",
    "SPARK_DOCKER_BASE = \"spark:3.5.7-scala2.12-java17-python3-ubuntu\"\n",
    "SPARK_JUPYTER_LAB_DOCKER_TAG = \"spark-jupyter:3.5.7-scala2.12-java17-python3-ubuntu\"\n",
    "SPARK_JOB_VENV_DOCKER_TAG = \"spark-job-venv:3.5.7-scala2.12-java17-python3-ubuntu\"\n",
    "SPARK_JOB_VENV_BUILD_DIR = \"/opt/spark/venv-build\"\n",
    "\n",
    "SPARK_MASTER_NAME = \"spark-master\"\n",
    "SPARK_MASTER_HOSTNAME = f\"{SPARK_MASTER_NAME}.mavasbel.vpn.itam.mx\"\n",
    "SPARK_MASTER_IP = \"10.15.20.2\"\n",
    "SPARK_MASTER_WUBUI_PORT = 6080\n",
    "SPARK_MASTER_PORT = 6077\n",
    "\n",
    "SPARK_TOTAL_WORKERS = 3\n",
    "SPARK_WORKER_NAMES = [f\"spark-worker-{i+1}\" for i in range(SPARK_TOTAL_WORKERS)]\n",
    "SPARK_WORKER_HOSTNAMES = [\n",
    "    f\"{SPARK_WORKER_NAMES[i]}.mavasbel.vpn.itam.mx\" for i in range(SPARK_TOTAL_WORKERS)\n",
    "]\n",
    "SPARK_WORKER_IPS = [\"10.15.20.2\"] * SPARK_TOTAL_WORKERS\n",
    "SPARK_WORKER_WEBUI_PORTS = [6080 + (i + 1) for i in range(SPARK_TOTAL_WORKERS)]\n",
    "\n",
    "SPARK_WORKDIR = \"/opt/spark/work-dir\"\n",
    "\n",
    "JUPYTER_LAB_NAME = \"spark-jupyter\"\n",
    "JUPYTER_LAB_HOSTNAME = \"spark-jupyter.mavasbel.vpn.itam.mx\"\n",
    "JUPYTER_LAB_IP = \"10.15.20.2\"\n",
    "JUPYTER_LAB_PORT = 6888\n",
    "JUPYTER_LAB_MONITOR_PORT = 4040\n",
    "JUPYTER_LAB_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a0a7937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HADOOP_NAMENODE_HOSTNAME = \"namenode.mavasbel.vpn.itam.mx\"\n",
    "HADOOP_NAMENODE_IP = \"10.15.20.2\"\n",
    "HADOOP_NAMENODE_PORT = 8020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52a9f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "LOCALHOST_WORKDIR = f\"{os.path.join(os.path.relpath(Path.cwd()))}\"\n",
    "DOCKER_MOUNTDIR = os.path.join(LOCALHOST_WORKDIR, \"mount\")\n",
    "\n",
    "path = Path(LOCALHOST_WORKDIR)\n",
    "path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c5fa77",
   "metadata": {},
   "source": [
    "# Stop spark-cluster.docker-compose.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "02a7cdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container spark-worker-3  Stopping\n",
      " Container spark-worker-3  Stopped\n",
      " Container spark-worker-3  Removing\n",
      " Container spark-worker-3  Removed\n",
      " Container spark-worker-2  Stopping\n",
      " Container spark-worker-2  Stopped\n",
      " Container spark-worker-2  Removing\n",
      " Container spark-worker-2  Removed\n",
      " Container spark-worker-1  Stopping\n",
      " Container spark-worker-1  Stopped\n",
      " Container spark-worker-1  Removing\n",
      " Container spark-worker-1  Removed\n",
      " Container spark-jupyter  Stopping\n",
      " Container spark-master  Stopping\n",
      " Container spark-master  Stopped\n",
      " Container spark-master  Removing\n",
      " Container spark-master  Removed\n",
      " Container spark-jupyter  Stopped\n",
      " Container spark-jupyter  Removing\n",
      " Container spark-jupyter  Removed\n",
      " Network spark-cluster_spark-cluster  Removing\n",
      " Network spark-cluster_spark-cluster  Removed\n"
     ]
    }
   ],
   "source": [
    "!docker compose -f spark-cluster.docker-compose.yml down -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c608a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if SPARK_START_FROM_SCRATCH or not os.path.exists(\n",
    "    os.path.join(DOCKER_MOUNTDIR, \"data\")\n",
    "):\n",
    "    shutil.rmtree(os.path.join(DOCKER_MOUNTDIR, \"data\"), ignore_errors=True)\n",
    "    shutil.rmtree(os.path.join(DOCKER_MOUNTDIR, \"spark-warehouse\"), ignore_errors=True)\n",
    "    shutil.rmtree(\n",
    "        os.path.join(DOCKER_MOUNTDIR, \"iceberg-warehouse\"), ignore_errors=True\n",
    "    )\n",
    "    shutil.rmtree(os.path.join(DOCKER_MOUNTDIR, SPARK_MASTER_NAME), ignore_errors=True)\n",
    "    for spark_worker_name in SPARK_WORKER_NAMES:\n",
    "        shutil.rmtree(\n",
    "            os.path.join(DOCKER_MOUNTDIR, spark_worker_name), ignore_errors=True\n",
    "        )\n",
    "\n",
    "    Path(os.path.join(DOCKER_MOUNTDIR, \"data\")).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b374a",
   "metadata": {},
   "source": [
    "### Build spark-jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "447180e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created: 'dockerfile.spark-jupyter'\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```dockerfile\n",
       " \n",
       "\n",
       "# Use the official Spark image as the base\n",
       "FROM apache/spark:3.5.7-scala2.12-java17-python3-ubuntu\n",
       "\n",
       "# Switch to root to install software\n",
       "USER root\n",
       "\n",
       "# Set the working directory\n",
       "WORKDIR /opt/spark/work-dir\n",
       "\n",
       "# Expose the Jupyter port\n",
       "EXPOSE 8888\n",
       "\n",
       "# Install Python dependencies\n",
       "RUN apt-get update && apt-get install -y python3-venv\n",
       "RUN python3 -m pip install --no-cache-dir pyspark==3.5.7 delta-spark==3.2.0 jupyterlab pandas pyarrow\n",
       "\n",
       "# Set the default command to launch Jupyter Lab\n",
       "CMD [\"jupyter\", \"lab\", \"--ip=0.0.0.0\", \"--port=8888\", \"--no-browser\", \"--allow-root\", \"--NotebookApp.token=$$JUPYTER_LAB_TOKEN\"]\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "dockerfile_spark_jupyter_python_packages = (\n",
    "    \"pyspark==3.5.7 delta-spark==3.2.0 jupyterlab pandas pyarrow\"\n",
    ")\n",
    "\n",
    "dockerfile_spark_jupyter_name = \"dockerfile.spark-jupyter\"\n",
    "\n",
    "# language=dockerfile\n",
    "dockerfile_spark_jupyter_contents = f\"\"\" \n",
    "\n",
    "# Use the official Spark image as the base\n",
    "FROM apache/{SPARK_DOCKER_BASE}\n",
    "\n",
    "# Switch to root to install software\n",
    "USER root\n",
    "\n",
    "# Set the working directory\n",
    "WORKDIR {SPARK_WORKDIR}\n",
    "\n",
    "# Expose the Jupyter port\n",
    "EXPOSE 8888\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN apt-get update && apt-get install -y python3-venv\n",
    "RUN python3 -m pip install --no-cache-dir {dockerfile_spark_jupyter_python_packages}\n",
    "\n",
    "# Set the default command to launch Jupyter Lab\n",
    "CMD [\"jupyter\", \"lab\", \"--ip=0.0.0.0\", \"--port=8888\", \"--no-browser\", \"--allow-root\", \"--NotebookApp.token=$$JUPYTER_LAB_TOKEN\"]\n",
    "\"\"\"\n",
    "\n",
    "with open(\n",
    "    os.path.join(LOCALHOST_WORKDIR, dockerfile_spark_jupyter_name), \"w\"\n",
    ") as spark_compose_yaml_file:\n",
    "    spark_compose_yaml_file.write(dockerfile_spark_jupyter_contents.strip())\n",
    "\n",
    "print(\n",
    "    f\"Successfully created: '{os.path.relpath(os.path.join(LOCALHOST_WORKDIR,dockerfile_spark_jupyter_name))}'\"\n",
    ")\n",
    "display(Markdown(f\"```dockerfile\\n{dockerfile_spark_jupyter_contents}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c705c40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0 building with \"desktop-linux\" instance using docker driver\n",
      "\n",
      "#1 [internal] load build definition from dockerfile.spark-jupyter\n",
      "#1 transferring dockerfile: 677B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load metadata for docker.io/apache/spark:3.5.7-scala2.12-java17-python3-ubuntu\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [internal] load .dockerignore\n",
      "#3 transferring context: 2B done\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [1/4] FROM docker.io/apache/spark:3.5.7-scala2.12-java17-python3-ubuntu\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [2/4] WORKDIR /opt/spark/work-dir\n",
      "#5 CACHED\n",
      "\n",
      "#6 [3/4] RUN apt-get update && apt-get install -y python3-venv\n",
      "#6 CACHED\n",
      "\n",
      "#7 [4/4] RUN python3 -m pip install --no-cache-dir pyspark==3.5.7 delta-spark==3.2.0 jupyterlab pandas pyarrow\n",
      "#7 CACHED\n",
      "\n",
      "#8 exporting to image\n",
      "#8 exporting layers done\n",
      "#8 writing image sha256:46a33deeab39c7bf9e01dd919d60bb5a70e9180456c6426246260c043416d7c8 done\n",
      "#8 naming to docker.io/library/spark-jupyter:3.5.7-scala2.12-java17-python3-ubuntu done\n",
      "#8 DONE 0.0s\n",
      "\n",
      "View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/kuww2wuru7tpe7ai9zt2zilim\n"
     ]
    }
   ],
   "source": [
    "!docker build -t {SPARK_JUPYTER_LAB_DOCKER_TAG} -f dockerfile.spark-jupyter ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbff5f3",
   "metadata": {},
   "source": [
    "### Build spark-job-venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "89dec358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created: 'dockerfile.spark-jupyter'\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```dockerfile\n",
       "\n",
       "# Use the previously generated spark-jupyter image as the base\n",
       "FROM spark-jupyter:3.5.7-scala2.12-java17-python3-ubuntu\n",
       "\n",
       "# Create virtual env for spark jobs\n",
       "RUN mkdir -p /opt/spark/venv-build && \\\n",
       "        cd /opt/spark/venv-build && \\\n",
       "        python3 -m venv --copies spark_job_env && \\\n",
       "        /opt/spark/venv-build/spark_job_env/bin/pip install venv-pack pandas pyarrow faker faker-commerce mimesis && \\\n",
       "        /opt/spark/venv-build/spark_job_env/bin/venv-pack -p spark_job_env -o spark_job_env.tar.gz\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "dockerfile_spark_job_venv_name = \"dockerfile.spark-job-venv\"\n",
    "dockerfile_spark_job_venv_contents = f\"\"\"\n",
    "# Use the previously generated spark-jupyter image as the base\n",
    "FROM {SPARK_JUPYTER_LAB_DOCKER_TAG}\n",
    "\n",
    "# Create virtual env for spark jobs\n",
    "RUN mkdir -p {SPARK_JOB_VENV_BUILD_DIR} && \\\\\n",
    "        cd {SPARK_JOB_VENV_BUILD_DIR} && \\\\\n",
    "        python3 -m venv --copies spark_job_env && \\\\\n",
    "        {SPARK_JOB_VENV_BUILD_DIR}/spark_job_env/bin/pip install venv-pack pandas pyarrow faker faker-commerce mimesis && \\\\\n",
    "        {SPARK_JOB_VENV_BUILD_DIR}/spark_job_env/bin/venv-pack -p spark_job_env -o spark_job_env.tar.gz\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(LOCALHOST_WORKDIR, dockerfile_spark_job_venv_name), \"w\") as spark_compose_yaml_file:\n",
    "    spark_compose_yaml_file.write(dockerfile_spark_job_venv_contents.strip())\n",
    "\n",
    "print(f\"Successfully created: '{os.path.relpath(os.path.join(LOCALHOST_WORKDIR,dockerfile_spark_jupyter_name))}'\")\n",
    "display(Markdown(f\"```dockerfile\\n{dockerfile_spark_job_venv_contents}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3423956e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0 building with \"desktop-linux\" instance using docker driver\n",
      "\n",
      "#1 [internal] load build definition from dockerfile.spark-job-venv\n",
      "#1 transferring dockerfile: 566B 0.0s done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load metadata for docker.io/library/spark-jupyter:3.5.7-scala2.12-java17-python3-ubuntu\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [internal] load .dockerignore\n",
      "#3 transferring context: 2B done\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [1/2] FROM docker.io/library/spark-jupyter:3.5.7-scala2.12-java17-python3-ubuntu\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [2/2] RUN mkdir -p /opt/spark/venv-build &&         cd /opt/spark/venv-build &&         python3 -m venv --copies spark_job_env &&         /opt/spark/venv-build/spark_job_env/bin/pip install venv-pack pandas pyarrow faker faker-commerce mimesis &&         /opt/spark/venv-build/spark_job_env/bin/venv-pack -p spark_job_env -o spark_job_env.tar.gz\n",
      "#5 CACHED\n",
      "\n",
      "#6 exporting to image\n",
      "#6 exporting layers done\n",
      "#6 writing image sha256:7398bf2b436d655d12125d7b6a95669a618aa6aad2a3d809b8f2ce03ef92b65d done\n",
      "#6 naming to docker.io/library/spark-job-venv:3.5.7-scala2.12-java17-python3-ubuntu done\n",
      "#6 DONE 0.0s\n",
      "\n",
      "View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/76zsct527360zzbx3qks85mn0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25cf9ddf89779e81cdcccdff0d287fcb613b8818d3bbc3af0208a6f38ad5b533\n",
      "spark-job-venv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "path = Path(DOCKER_MOUNTDIR)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "!docker build -t {SPARK_JOB_VENV_DOCKER_TAG} -f dockerfile.spark-job-venv .\n",
    "!docker create --name spark-job-venv {SPARK_JOB_VENV_DOCKER_TAG}\n",
    "!docker cp spark-job-venv:{SPARK_JOB_VENV_BUILD_DIR}/spark_job_env.tar.gz \"{DOCKER_MOUNTDIR}/spark_job_env.tar.gz\"\n",
    "!docker rm spark-job-venv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7453ce",
   "metadata": {},
   "source": [
    "# Start spark.docker-compose.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b6cd7834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created: 'spark-cluster.docker-compose.yml'\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "name: spark-cluster\n",
       "networks:\n",
       "    spark-cluster:\n",
       "        driver: bridge\n",
       "services:\n",
       "    spark-master:\n",
       "        image: apache/spark:3.5.7-scala2.12-java17-python3-ubuntu\n",
       "        container_name: spark-master\n",
       "        user: root\n",
       "        command: bash -c \"/opt/spark/bin/spark-class org.apache.spark.deploy.$$SPARK_MODE.$${SPARK_MODE^}\n",
       "            --host spark-master.mavasbel.vpn.itam.mx --port $$SPARK_MASTER_PORT --webui-port\n",
       "            $$SPARK_MASTER_WEBUI_PORT\"\n",
       "        environment:\n",
       "        - PYSPARK_PYTHON=python3\n",
       "        - SPARK_MODE=master\n",
       "        - SPARK_MASTER_PORT=6077\n",
       "        - SPARK_MASTER_WEBUI_PORT=6080\n",
       "        - SPARK_DAEMON_MEMORY=1G\n",
       "        volumes:\n",
       "        - .\\mount\\spark-master:/opt/spark/work-dir\n",
       "        - .\\jars\\iceberg-spark-runtime-3.5_2.12-1.6.1.jar:/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.6.1.jar\n",
       "        networks:\n",
       "        - spark-cluster\n",
       "        hostname: spark-master.mavasbel.vpn.itam.mx\n",
       "        ports:\n",
       "        - 6080:6080\n",
       "        - 6077:6077\n",
       "        extra_hosts:\n",
       "        - host.docker.internal:host-gateway\n",
       "        dns: &id001\n",
       "        - 10.15.20.1\n",
       "        deploy:\n",
       "            resources:\n",
       "                limits:\n",
       "                    cpus: '2.0'\n",
       "                    memory: 1024M\n",
       "        healthcheck:\n",
       "            test:\n",
       "            - CMD\n",
       "            - curl\n",
       "            - -f\n",
       "            - http://spark-master.mavasbel.vpn.itam.mx:6080\n",
       "            interval: 10s\n",
       "            timeout: 10s\n",
       "            retries: 10\n",
       "            start_period: 10s\n",
       "    spark-jupyter:\n",
       "        image: spark-jupyter:3.5.7-scala2.12-java17-python3-ubuntu\n",
       "        container_name: spark-jupyter\n",
       "        user: root\n",
       "        command:\n",
       "        - bash\n",
       "        - -c\n",
       "        - jupyter lab --ip=0.0.0.0 --port=6888 --no-browser --allow-root --NotebookApp.token=''\n",
       "            --NotebookApp.password='' --NotebookApp.allow_origin='*' --ServerApp.disable_check_xsrf=True\n",
       "            --ServerApp.root_dir=/opt/spark/work-dir\n",
       "        environment:\n",
       "        - PYSPARK_PYTHON=python3\n",
       "        - SPARK_EXECUTOR_MEMORY=1536M\n",
       "        volumes:\n",
       "        - .\\mount:/opt/spark/work-dir\n",
       "        - .\\vscode_server:/root/.vscode-server\n",
       "        - .\\jars\\iceberg-spark-runtime-3.5_2.12-1.6.1.jar:/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.6.1.jar\n",
       "        networks:\n",
       "        - spark-cluster\n",
       "        hostname: spark-jupyter.mavasbel.vpn.itam.mx\n",
       "        ports:\n",
       "        - 6888:6888\n",
       "        - 4040:4040\n",
       "        extra_hosts:\n",
       "        - host.docker.internal:host-gateway\n",
       "        dns: *id001\n",
       "        deploy:\n",
       "            resources:\n",
       "                limits:\n",
       "                    cpus: '2.0'\n",
       "                    memory: 2048M\n",
       "        healthcheck:\n",
       "            test:\n",
       "            - CMD\n",
       "            - curl\n",
       "            - -f\n",
       "            - http://spark-jupyter.mavasbel.vpn.itam.mx:6888\n",
       "            interval: 10s\n",
       "            timeout: 10s\n",
       "            retries: 10\n",
       "            start_period: 10s\n",
       "    spark-worker-1:\n",
       "        image: apache/spark:3.5.7-scala2.12-java17-python3-ubuntu\n",
       "        container_name: spark-worker-1\n",
       "        user: root\n",
       "        command: bash -c \"/opt/spark/bin/spark-class org.apache.spark.deploy.$$SPARK_MODE.$${SPARK_MODE^}\n",
       "            $$SPARK_MASTER_URL --host spark-worker-1.mavasbel.vpn.itam.mx --webui-port\n",
       "            $$SPARK_WORKER_WEBUI_PORT\"\n",
       "        environment:\n",
       "        - PYSPARK_PYTHON=python3\n",
       "        - SPARK_MODE=worker\n",
       "        - SPARK_WORKER_CORES=2\n",
       "        - SPARK_DAEMON_MEMORY=512M\n",
       "        - SPARK_WORKER_MEMORY=2048M\n",
       "        - SPARK_WORKER_WEBUI_PORT=6081\n",
       "        - SPARK_MASTER_URL=spark://spark-master.mavasbel.vpn.itam.mx:6077\n",
       "        volumes:\n",
       "        - .\\mount\\spark-worker-1:/opt/spark/work-dir\n",
       "        - .\\jars\\iceberg-spark-runtime-3.5_2.12-1.6.1.jar:/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.6.1.jar\n",
       "        networks:\n",
       "        - spark-cluster\n",
       "        hostname: spark-worker-1.mavasbel.vpn.itam.mx\n",
       "        ports:\n",
       "        - 6081:6081\n",
       "        extra_hosts:\n",
       "        - host.docker.internal:host-gateway\n",
       "        dns: *id001\n",
       "        deploy:\n",
       "            resources:\n",
       "                limits:\n",
       "                    cpus: '2.0'\n",
       "                    memory: 3G\n",
       "        depends_on:\n",
       "            spark-master:\n",
       "                condition: service_healthy\n",
       "            spark-jupyter:\n",
       "                condition: service_healthy\n",
       "        healthcheck:\n",
       "            test:\n",
       "            - CMD\n",
       "            - curl\n",
       "            - -f\n",
       "            - http://spark-worker-1.mavasbel.vpn.itam.mx:6081\n",
       "            interval: 10s\n",
       "            timeout: 10s\n",
       "            retries: 10\n",
       "            start_period: 10s\n",
       "    spark-worker-2:\n",
       "        image: apache/spark:3.5.7-scala2.12-java17-python3-ubuntu\n",
       "        container_name: spark-worker-2\n",
       "        user: root\n",
       "        command: bash -c \"/opt/spark/bin/spark-class org.apache.spark.deploy.$$SPARK_MODE.$${SPARK_MODE^}\n",
       "            $$SPARK_MASTER_URL --host spark-worker-2.mavasbel.vpn.itam.mx --webui-port\n",
       "            $$SPARK_WORKER_WEBUI_PORT\"\n",
       "        environment:\n",
       "        - PYSPARK_PYTHON=python3\n",
       "        - SPARK_MODE=worker\n",
       "        - SPARK_WORKER_CORES=2\n",
       "        - SPARK_DAEMON_MEMORY=512M\n",
       "        - SPARK_WORKER_MEMORY=2048M\n",
       "        - SPARK_WORKER_WEBUI_PORT=6082\n",
       "        - SPARK_MASTER_URL=spark://spark-master.mavasbel.vpn.itam.mx:6077\n",
       "        volumes:\n",
       "        - .\\mount\\spark-worker-2:/opt/spark/work-dir\n",
       "        - .\\jars\\iceberg-spark-runtime-3.5_2.12-1.6.1.jar:/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.6.1.jar\n",
       "        networks:\n",
       "        - spark-cluster\n",
       "        hostname: spark-worker-2.mavasbel.vpn.itam.mx\n",
       "        ports:\n",
       "        - 6082:6082\n",
       "        extra_hosts:\n",
       "        - host.docker.internal:host-gateway\n",
       "        dns: *id001\n",
       "        deploy:\n",
       "            resources:\n",
       "                limits:\n",
       "                    cpus: '2.0'\n",
       "                    memory: 3G\n",
       "        depends_on:\n",
       "            spark-master:\n",
       "                condition: service_healthy\n",
       "            spark-jupyter:\n",
       "                condition: service_healthy\n",
       "            spark-worker-1:\n",
       "                condition: service_started\n",
       "        healthcheck:\n",
       "            test:\n",
       "            - CMD\n",
       "            - curl\n",
       "            - -f\n",
       "            - http://spark-worker-2.mavasbel.vpn.itam.mx:6082\n",
       "            interval: 10s\n",
       "            timeout: 10s\n",
       "            retries: 10\n",
       "            start_period: 10s\n",
       "    spark-worker-3:\n",
       "        image: apache/spark:3.5.7-scala2.12-java17-python3-ubuntu\n",
       "        container_name: spark-worker-3\n",
       "        user: root\n",
       "        command: bash -c \"/opt/spark/bin/spark-class org.apache.spark.deploy.$$SPARK_MODE.$${SPARK_MODE^}\n",
       "            $$SPARK_MASTER_URL --host spark-worker-3.mavasbel.vpn.itam.mx --webui-port\n",
       "            $$SPARK_WORKER_WEBUI_PORT\"\n",
       "        environment:\n",
       "        - PYSPARK_PYTHON=python3\n",
       "        - SPARK_MODE=worker\n",
       "        - SPARK_WORKER_CORES=2\n",
       "        - SPARK_DAEMON_MEMORY=512M\n",
       "        - SPARK_WORKER_MEMORY=2048M\n",
       "        - SPARK_WORKER_WEBUI_PORT=6083\n",
       "        - SPARK_MASTER_URL=spark://spark-master.mavasbel.vpn.itam.mx:6077\n",
       "        volumes:\n",
       "        - .\\mount\\spark-worker-3:/opt/spark/work-dir\n",
       "        - .\\jars\\iceberg-spark-runtime-3.5_2.12-1.6.1.jar:/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.6.1.jar\n",
       "        networks:\n",
       "        - spark-cluster\n",
       "        hostname: spark-worker-3.mavasbel.vpn.itam.mx\n",
       "        ports:\n",
       "        - 6083:6083\n",
       "        extra_hosts:\n",
       "        - host.docker.internal:host-gateway\n",
       "        dns: *id001\n",
       "        deploy:\n",
       "            resources:\n",
       "                limits:\n",
       "                    cpus: '2.0'\n",
       "                    memory: 3G\n",
       "        depends_on:\n",
       "            spark-master:\n",
       "                condition: service_healthy\n",
       "            spark-jupyter:\n",
       "                condition: service_healthy\n",
       "            spark-worker-1:\n",
       "                condition: service_started\n",
       "            spark-worker-2:\n",
       "                condition: service_started\n",
       "        healthcheck:\n",
       "            test:\n",
       "            - CMD\n",
       "            - curl\n",
       "            - -f\n",
       "            - http://spark-worker-3.mavasbel.vpn.itam.mx:6083\n",
       "            interval: 10s\n",
       "            timeout: 10s\n",
       "            retries: 10\n",
       "            start_period: 10s\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "SPARK_VSCODE_SERVER_DIR = os.path.join(LOCALHOST_WORKDIR, \"vscode_server\")\n",
    "SPARK_MOUNT_JARS = [\n",
    "    f\"{os.path.join(LOCALHOST_WORKDIR,\"jars\",file)}:/opt/spark/jars/{file}\"\n",
    "    for file in os.listdir(os.path.join(LOCALHOST_WORKDIR, \"jars\"))\n",
    "    if file.endswith(\"jar\")\n",
    "]\n",
    "\n",
    "spark_compose_dict = {\n",
    "    \"name\": \"spark-cluster\",\n",
    "    \"networks\": {\"spark-cluster\": {\"driver\": \"bridge\"}},\n",
    "    \"services\": {\n",
    "        SPARK_MASTER_NAME: {\n",
    "            \"image\": f\"apache/{SPARK_DOCKER_BASE}\",\n",
    "            \"container_name\": SPARK_MASTER_NAME,\n",
    "            \"user\": \"root\",\n",
    "            \"command\": f'bash -c \"/opt/spark/bin/spark-class org.apache.spark.deploy.$$SPARK_MODE.$${{SPARK_MODE^}} --host {SPARK_MASTER_HOSTNAME} --port $$SPARK_MASTER_PORT --webui-port $$SPARK_MASTER_WEBUI_PORT\"',\n",
    "            \"environment\": [\n",
    "                \"PYSPARK_PYTHON=python3\",\n",
    "                \"SPARK_MODE=master\",\n",
    "                f\"SPARK_MASTER_PORT={SPARK_MASTER_PORT}\",\n",
    "                f\"SPARK_MASTER_WEBUI_PORT={SPARK_MASTER_WUBUI_PORT}\",\n",
    "                \"SPARK_DAEMON_MEMORY=1G\",\n",
    "            ],\n",
    "            \"volumes\": [\n",
    "                f\"{os.path.join(DOCKER_MOUNTDIR,SPARK_MASTER_NAME)}:{SPARK_WORKDIR}\"\n",
    "            ]\n",
    "            + SPARK_MOUNT_JARS,\n",
    "            \"networks\": [\"spark-cluster\"],\n",
    "            \"hostname\": SPARK_MASTER_HOSTNAME,\n",
    "            \"ports\": [\n",
    "                f\"{SPARK_MASTER_WUBUI_PORT}:{SPARK_MASTER_WUBUI_PORT}\",\n",
    "                f\"{SPARK_MASTER_PORT}:{SPARK_MASTER_PORT}\",\n",
    "            ],\n",
    "            \"extra_hosts\": [\n",
    "                f\"{DOCKER_INTERNAL_HOST}:host-gateway\",\n",
    "            ],\n",
    "            \"dns\": DOCKER_DNS,\n",
    "            \"deploy\": {\"resources\": {\"limits\": {\"cpus\": \"2.0\", \"memory\": \"1024M\"}}},\n",
    "            \"healthcheck\": {\n",
    "                \"test\": [\n",
    "                    \"CMD\",\n",
    "                    \"curl\",\n",
    "                    \"-f\",\n",
    "                    f\"http://{SPARK_MASTER_HOSTNAME}:{SPARK_MASTER_WUBUI_PORT}\",\n",
    "                ],\n",
    "                \"interval\": \"10s\",\n",
    "                \"timeout\": \"10s\",\n",
    "                \"retries\": 10,\n",
    "                \"start_period\": \"10s\",\n",
    "            },\n",
    "        },\n",
    "        \"spark-jupyter\": {\n",
    "            \"image\": SPARK_JUPYTER_LAB_DOCKER_TAG,\n",
    "            \"container_name\": \"spark-jupyter\",\n",
    "            \"user\": \"root\",\n",
    "            \"command\": [\n",
    "                \"bash\",\n",
    "                \"-c\",\n",
    "                \" \".join(\n",
    "                    [\n",
    "                        \"jupyter lab\",\n",
    "                        \"--ip=0.0.0.0\",\n",
    "                        f\"--port={JUPYTER_LAB_PORT}\",\n",
    "                        \"--no-browser\",\n",
    "                        \"--allow-root\",\n",
    "                        f\"--NotebookApp.token='{JUPYTER_LAB_TOKEN}'\",\n",
    "                        \"--NotebookApp.password=''\",\n",
    "                        \"--NotebookApp.allow_origin='*'\",\n",
    "                        \"--ServerApp.disable_check_xsrf=True\",\n",
    "                        f\"--ServerApp.root_dir={SPARK_WORKDIR}\",\n",
    "                    ]\n",
    "                ),\n",
    "            ],\n",
    "            \"environment\": [\n",
    "                \"PYSPARK_PYTHON=python3\",\n",
    "                # f\"JUPYTER_LAB_PORT={JUPYTER_LAB_PORT}\",\n",
    "                # f\"JUPYTER_LAB_TOKEN={JUPYTER_LAB_TOKEN}\",\n",
    "                \"SPARK_EXECUTOR_MEMORY=1536M\",\n",
    "            ],\n",
    "            \"volumes\": [\n",
    "                f\"{DOCKER_MOUNTDIR}:{SPARK_WORKDIR}\",\n",
    "                f\"{SPARK_VSCODE_SERVER_DIR}:/root/.vscode-server\",\n",
    "            ]\n",
    "            + SPARK_MOUNT_JARS,\n",
    "            \"networks\": [\"spark-cluster\"],\n",
    "            \"hostname\": JUPYTER_LAB_HOSTNAME,\n",
    "            \"ports\": [\n",
    "                f\"{JUPYTER_LAB_PORT}:{JUPYTER_LAB_PORT}\",\n",
    "                f\"{JUPYTER_LAB_MONITOR_PORT}:{JUPYTER_LAB_MONITOR_PORT}\",\n",
    "            ],\n",
    "            \"extra_hosts\": [\n",
    "                f\"{DOCKER_INTERNAL_HOST}:host-gateway\",\n",
    "            ],\n",
    "            \"dns\": DOCKER_DNS,\n",
    "            \"deploy\": {\"resources\": {\"limits\": {\"cpus\": \"2.0\", \"memory\": \"2048M\"}}},\n",
    "            \"healthcheck\": {\n",
    "                \"test\": [\n",
    "                    \"CMD\",\n",
    "                    \"curl\",\n",
    "                    \"-f\",\n",
    "                    f\"http://{JUPYTER_LAB_HOSTNAME}:{JUPYTER_LAB_PORT}\",\n",
    "                ],\n",
    "                \"interval\": \"10s\",\n",
    "                \"timeout\": \"10s\",\n",
    "                \"retries\": 10,\n",
    "                \"start_period\": \"10s\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "for i in range(SPARK_TOTAL_WORKERS):\n",
    "\n",
    "    spark_compose_dict[\"services\"][SPARK_WORKER_NAMES[i]] = {\n",
    "        \"image\": f\"apache/{SPARK_DOCKER_BASE}\",\n",
    "        \"container_name\": SPARK_WORKER_NAMES[i],\n",
    "        \"user\": \"root\",\n",
    "        \"command\": f'bash -c \"/opt/spark/bin/spark-class org.apache.spark.deploy.$$SPARK_MODE.$${{SPARK_MODE^}} $$SPARK_MASTER_URL --host {SPARK_WORKER_HOSTNAMES[i]} --webui-port $$SPARK_WORKER_WEBUI_PORT\"',\n",
    "        \"environment\": [\n",
    "            \"PYSPARK_PYTHON=python3\",\n",
    "            \"SPARK_MODE=worker\",\n",
    "            \"SPARK_WORKER_CORES=2\",\n",
    "            \"SPARK_DAEMON_MEMORY=512M\",\n",
    "            \"SPARK_WORKER_MEMORY=2048M\",\n",
    "            f\"SPARK_WORKER_WEBUI_PORT={SPARK_WORKER_WEBUI_PORTS[i]}\",\n",
    "            f\"SPARK_MASTER_URL=spark://{SPARK_MASTER_HOSTNAME}:{SPARK_MASTER_PORT}\",\n",
    "        ],\n",
    "        \"volumes\": [\n",
    "            f\"{os.path.join(DOCKER_MOUNTDIR,SPARK_WORKER_NAMES[i])}:{SPARK_WORKDIR}\"\n",
    "        ]\n",
    "        + SPARK_MOUNT_JARS,\n",
    "        \"networks\": [\"spark-cluster\"],\n",
    "        \"hostname\": SPARK_WORKER_HOSTNAMES[i],\n",
    "        \"ports\": [f\"{SPARK_WORKER_WEBUI_PORTS[i]}:{SPARK_WORKER_WEBUI_PORTS[i]}\"],\n",
    "        \"extra_hosts\": [\n",
    "            f\"{DOCKER_INTERNAL_HOST}:host-gateway\",\n",
    "        ],\n",
    "        \"dns\": DOCKER_DNS,\n",
    "        \"deploy\": {\"resources\": {\"limits\": {\"cpus\": \"2.0\", \"memory\": \"3G\"}}},\n",
    "        \"depends_on\": {\n",
    "            \"spark-master\": {\"condition\": \"service_healthy\"},\n",
    "            \"spark-jupyter\": {\"condition\": \"service_healthy\"},\n",
    "        }\n",
    "        | {\n",
    "            SPARK_WORKER_NAMES[j]: {\"condition\": \"service_started\"} for j in range(0, i)\n",
    "        },\n",
    "        \"healthcheck\": {\n",
    "            \"test\": [\n",
    "                \"CMD\",\n",
    "                \"curl\",\n",
    "                \"-f\",\n",
    "                f\"http://{SPARK_WORKER_HOSTNAMES[i]}:{SPARK_WORKER_WEBUI_PORTS[i]}\",\n",
    "            ],\n",
    "            \"interval\": \"10s\",\n",
    "            \"timeout\": \"10s\",\n",
    "            \"retries\": 10,\n",
    "            \"start_period\": \"10s\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "# 3. Dump the dictionary to a YAML file\n",
    "spark_compose_yaml_path = os.path.join(\n",
    "    LOCALHOST_WORKDIR, \"spark-cluster.docker-compose.yml\"\n",
    ")\n",
    "spark_compose_yaml_contents = yaml.dump(\n",
    "    spark_compose_dict, default_flow_style=False, sort_keys=False, indent=4\n",
    ")\n",
    "with open(spark_compose_yaml_path, \"w\") as spark_compose_yaml_file:\n",
    "    spark_compose_yaml_file.write(spark_compose_yaml_contents)\n",
    "\n",
    "print(f\"Successfully created: '{os.path.relpath(spark_compose_yaml_path)}'\")\n",
    "display(Markdown(f\"```yaml\\n{spark_compose_yaml_contents}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4867941a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Network spark-cluster_spark-cluster  Creating\n",
      " Network spark-cluster_spark-cluster  Created\n",
      " Container spark-jupyter  Creating\n",
      " Container spark-master  Creating\n",
      " Container spark-master  Created\n",
      " Container spark-jupyter  Created\n",
      " Container spark-worker-1  Creating\n",
      " Container spark-worker-1  Created\n",
      " Container spark-worker-2  Creating\n",
      " Container spark-worker-2  Created\n",
      " Container spark-worker-3  Creating\n",
      " Container spark-worker-3  Created\n",
      " Container spark-jupyter  Starting\n",
      " Container spark-master  Starting\n",
      " Container spark-master  Started\n",
      " Container spark-jupyter  Started\n",
      " Container spark-master  Waiting\n",
      " Container spark-jupyter  Waiting\n",
      " Container spark-jupyter  Healthy\n",
      " Container spark-master  Healthy\n",
      " Container spark-worker-1  Starting\n",
      " Container spark-worker-1  Started\n",
      " Container spark-jupyter  Waiting\n",
      " Container spark-master  Waiting\n",
      " Container spark-jupyter  Healthy\n",
      " Container spark-master  Healthy\n",
      " Container spark-worker-2  Starting\n",
      " Container spark-worker-2  Started\n",
      " Container spark-master  Waiting\n",
      " Container spark-jupyter  Waiting\n",
      " Container spark-jupyter  Healthy\n",
      " Container spark-master  Healthy\n",
      " Container spark-worker-3  Starting\n",
      " Container spark-worker-3  Started\n",
      " Container spark-worker-1  Waiting\n",
      " Container spark-worker-2  Waiting\n",
      " Container spark-worker-3  Waiting\n",
      " Container spark-master  Waiting\n",
      " Container spark-jupyter  Waiting\n",
      " Container spark-master  Healthy\n",
      " Container spark-jupyter  Healthy\n",
      " Container spark-worker-1  Healthy\n",
      " Container spark-worker-2  Healthy\n",
      " Container spark-worker-3  Healthy\n"
     ]
    }
   ],
   "source": [
    "!docker compose -f spark-cluster.docker-compose.yml up -d --wait"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "non-relational-dbs-labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
