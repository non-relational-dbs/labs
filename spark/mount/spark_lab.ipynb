{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73bdf2f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc599dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_START_FROM_SCRATCH = True\n",
    "DOCKER_INTERNAL_HOST = \"host.docker.internal\"\n",
    "DOCKER_DNS = [\"10.15.20.1\"]\n",
    "\n",
    "SPARK_DOCKER_BASE = \"spark:3.5.7-scala2.12-java17-python3-ubuntu\"\n",
    "SPARK_JUPYTER_LAB_DOCKER_TAG = \"spark-jupyter:3.5.7-scala2.12-java17-python3-ubuntu\"\n",
    "SPARK_JOB_VENV_DOCKER_TAG = \"spark-job-venv:3.5.7-scala2.12-java17-python3-ubuntu\"\n",
    "SPARK_JOB_VENV_BUILD_DIR = \"/opt/spark/venv-build\"\n",
    "\n",
    "SPARK_MASTER_NAME = \"spark-master\"\n",
    "SPARK_MASTER_HOSTNAME = f\"{SPARK_MASTER_NAME}.mavasbel.vpn.itam.mx\"\n",
    "SPARK_MASTER_IP = \"10.15.20.2\"\n",
    "SPARK_MASTER_WUBUI_PORT = 6080\n",
    "SPARK_MASTER_PORT = 6077\n",
    "\n",
    "SPARK_TOTAL_WORKERS = 3\n",
    "SPARK_WORKER_NAMES = [f\"spark-worker-{i+1}\" for i in range(SPARK_TOTAL_WORKERS)]\n",
    "SPARK_WORKER_HOSTNAMES = [\n",
    "    f\"{SPARK_WORKER_NAMES[i]}.mavasbel.vpn.itam.mx\" for i in range(SPARK_TOTAL_WORKERS)\n",
    "]\n",
    "SPARK_WORKER_IPS = [\"10.15.20.2\"] * SPARK_TOTAL_WORKERS\n",
    "SPARK_WORKER_WEBUI_PORTS = [6080 + (i + 1) for i in range(SPARK_TOTAL_WORKERS)]\n",
    "\n",
    "SPARK_WORKDIR = \"/opt/spark/work-dir\"\n",
    "\n",
    "JUPYTER_LAB_NAME = \"spark-jupyter\"\n",
    "JUPYTER_LAB_HOSTNAME = \"spark-jupyter.mavasbel.vpn.itam.mx\"\n",
    "JUPYTER_LAB_IP = \"10.15.20.2\"\n",
    "JUPYTER_LAB_PORT = 6888\n",
    "JUPYTER_LAB_MONITOR_PORT = 4040\n",
    "JUPYTER_LAB_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48c5c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "HADOOP_NAMENODE_HOSTNAME = \"namenode.mavasbel.vpn.itam.mx\"\n",
    "HADOOP_NAMENODE_IP = \"10.15.20.2\"\n",
    "HADOOP_NAMENODE_PORT = 8020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3934165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "SPARK_DATADIR = Path(os.path.join(os.path.abspath(Path.cwd()), \"data\"))\n",
    "SPARK_DATADIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebd79568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (40.1.0)\n",
      "Requirement already satisfied: mimesis in /usr/local/lib/python3.10/dist-packages (19.1.0)\n",
      "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from faker) (2025.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install faker mimesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0914f8e5",
   "metadata": {},
   "source": [
    "##### Cleaning Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de195dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Spark Version: 3.5.7\n",
      "    PySpark Version: 3.5.7\n",
      "ðŸ§¹ Ghost SparkContext cleaned up.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "print(f\"      Spark Version: {sc.version}\")\n",
    "print(f\"    PySpark Version: {pyspark.__version__}\")\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "    print(\"ðŸ§¹ Ghost SparkContext cleaned up.\")\n",
    "except Exception:\n",
    "    print(\"âœ¨ No existing SparkContext to clean.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52f50c5",
   "metadata": {},
   "source": [
    "# Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "deaf8ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.7\n",
      "Scala Version: version 2.12.18\n",
      "âœ… Spark Session is now active.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "SparkSession.builder.getOrCreate().stop()\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.master(f\"spark://{SPARK_MASTER_HOSTNAME}:{SPARK_MASTER_PORT}\")\n",
    "    .appName(f\"SparkLab_{datetime.now().strftime('%Y-%m-%d_%H:%M:%S.%f')}\")\n",
    "    .config(\"spark.archives\", f\"{SPARK_WORKDIR}/spark_job_env.tar.gz#environment\")\n",
    "    .config(\"spark.driver.host\", f\"{JUPYTER_LAB_HOSTNAME}\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .config(\"spark.driver.memory\", \"512m\")\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", \"./environment/bin/python3\")\n",
    "    .config(\"spark.executor.memory\", \"1G\")\n",
    "    .config(\n",
    "        \"spark.executorEnv.PYTHONPATH\",\n",
    "        f\"./environment/lib/python{'.'.join(str(n) for n in sys.version_info[:2])}/site-packages\",\n",
    "    )\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", os.path.abspath(\"iceberg-warehouse\"))\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\")\n",
    "    .config(\"spark.sql.warehouse.dir\", os.path.abspath(\"spark-warehouse\"))\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    )\n",
    "    .enableHiveSupport()\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Scala Version: {spark._jvm.scala.util.Properties.versionString()}\")\n",
    "print(\"âœ… Spark Session is now active.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c31f4b",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720eca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = 10000\n",
    "partitions = 10\n",
    "\n",
    "\n",
    "def batch_generator(ids):\n",
    "    import socket\n",
    "    import random\n",
    "    from faker import Faker\n",
    "\n",
    "    node_name = socket.gethostname()\n",
    "    faker = Faker()\n",
    "    for _ in ids:\n",
    "        yield (\n",
    "            faker.uuid4(),\n",
    "            node_name,\n",
    "            faker.date_time(),\n",
    "            faker.first_name(),\n",
    "            faker.last_name(),\n",
    "            faker.email(),\n",
    "            faker.basic_phone_number(),\n",
    "            random.random() * 1000.0,\n",
    "        )\n",
    "\n",
    "df_column_names = [\n",
    "    \"id\",\n",
    "    \"worker\",\n",
    "    \"timestamp\",\n",
    "    \"first_name\",\n",
    "    \"last_name\",\n",
    "    \"email\",\n",
    "    \"phone\",\n",
    "    \"amount\",\n",
    "]\n",
    "df_column_types = spark.createDataFrame(\n",
    "    list(batch_generator(range(1))), schema=df_column_names\n",
    ").schema\n",
    "print(f\"âœ… batch_generator schema: {df_column_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30135100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    list(batch_generator(range(total_rows))), df_column_names\n",
    ").repartition(partitions)\n",
    "df.write.mode(\"overwrite\").csv(f\"{SPARK_DATADIR}/faker.csv\")\n",
    "print(f\"âœ… Created {SPARK_DATADIR}/faker.csv\")\n",
    "\n",
    "partition_stats = (\n",
    "    df.withColumn(\"partition_id\", F.spark_partition_id())\n",
    "    .groupBy(\"worker\", \"partition_id\")\n",
    "    .count()\n",
    "    .orderBy(\"worker\", \"partition_id\")\n",
    ")\n",
    "# partition_stats.show()\n",
    "display(partition_stats.toPandas())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5153d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(range(total_rows), partitions).mapPartitions(\n",
    "    batch_generator\n",
    ")\n",
    "df = rdd.toDF(df_column_names)\n",
    "df.write.mode(\"overwrite\").parquet(f\"{SPARK_DATADIR}/faker.parquet\")\n",
    "print(f\"âœ… Created {SPARK_DATADIR}/faker.parquet\")\n",
    "\n",
    "partition_stats = (\n",
    "    df.withColumn(\"partition_id\", F.spark_partition_id())\n",
    "    .groupBy(\"worker\", \"partition_id\")\n",
    "    .count()\n",
    "    .orderBy(\"worker\", \"partition_id\")\n",
    ")\n",
    "# partition_stats.show()\n",
    "display(partition_stats.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@pandas_udf(df_column_types)\n",
    "def generate_batch_vectorized(batch_ser: pd.Series) -> pd.DataFrame:\n",
    "    return pd.DataFrame(list(batch_generator(batch_ser)))\n",
    "\n",
    "\n",
    "df: DataFrame = (\n",
    "    spark.range(total_rows, numPartitions=partitions)\n",
    "    .withColumn(\"data\", generate_batch_vectorized(\"id\"))\n",
    "    .select(\"data.*\")\n",
    ")\n",
    "# df.write.mode(\"overwrite\").parquet(f\"{SPARK_DATADIR}/faker_vectorized.parquet\")\n",
    "# df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{SPARK_DATADIR}/faker_vectorized.parquet\")\n",
    "# pdf = df.toPandas()\n",
    "# pdf.to_parquet(f\"{SPARK_DATADIR}/faker_vectorized.parquet\", index=False)\n",
    "df.write.mode(\"overwrite\").parquet(f\"hdfs://{HADOOP_NAMENODE_HOSTNAME}:{HADOOP_NAMENODE_PORT}/spark/work-dir/faker_vectorized.parquet\")\n",
    "print(f\"âœ… Created hdfs://{HADOOP_NAMENODE_HOSTNAME}:{HADOOP_NAMENODE_PORT}/spark/work-dir/faker_vectorized.parquet\")\n",
    "\n",
    "\n",
    "partition_stats = (\n",
    "    df.withColumn(\"partition_id\", F.spark_partition_id())\n",
    "    .groupBy(\"worker\", \"partition_id\")\n",
    "    .count()\n",
    "    .orderBy(\"worker\", \"partition_id\")\n",
    ")\n",
    "# partition_stats.show()\n",
    "display(partition_stats.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214583cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Read it back and check the schema/count\n",
    "\n",
    "# df_verify = spark.read.parquet(f\"{SPARK_DATADIR}/faker_vectorized.parquet\").repartition(partitions)\n",
    "# pdf_verify = pd.read_parquet(f\"{SPARK_DATADIR}/faker_vectorized.parquet\")\n",
    "# df_verify = spark.createDataFrame(pdf_verify).repartition(partitions)\n",
    "df_verify = spark.read.parquet(f\"hdfs://{HADOOP_NAMENODE_HOSTNAME}:{HADOOP_NAMENODE_PORT}/spark/work-dir/faker_vectorized.parquet\").repartition(partitions)\n",
    "print(f\"Generated rows: {df_verify.count()}\")\n",
    "\n",
    "print(\"\\nFirst 10 by timestamp desc:\")\n",
    "# df_verify.sort(F.col(\"timestamp\").desc()).show(10)\n",
    "display(df_verify.sort(F.col(\"timestamp\").desc()).toPandas())\n",
    "\n",
    "print(\"\\nFirst 10 by count(first_name) desc:\")\n",
    "# df_verify.groupBy(\"first_name\").count().sort(F.col(\"count\").desc()).show(10)\n",
    "display(df_verify.groupBy(\"first_name\").count().sort(F.col(\"count\").desc()).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb52eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "df_verify.createOrReplaceTempView(\"df_verify\")\n",
    "df_sparkql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        first_name, \n",
    "        SUM(amount) as total_amount,\n",
    "        COUNT(*) as first_name_count\n",
    "    FROM df_verify\n",
    "    GROUP BY first_name\n",
    "    ORDER BY first_name_count DESC\n",
    "\"\"\")\n",
    "display(df_sparkql.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e179ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "df_sparkql = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        first_name, \n",
    "        SUM(amount) as total_amount,\n",
    "        COUNT(*) as first_name_count\n",
    "    FROM parquet.`hdfs://{HADOOP_NAMENODE_HOSTNAME}:{HADOOP_NAMENODE_PORT}/spark/work-dir/faker_vectorized.parquet`\n",
    "    GROUP BY first_name\n",
    "    ORDER BY first_name_count DESC\n",
    "\"\"\")\n",
    "display(df_sparkql.toPandas())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
